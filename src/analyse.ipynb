{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fed8a-8fc9-47c7-a77b-79a209da73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# some tsv were too big, so increase max memory allocation of csv module\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd4287-0674-47b6-bd6c-0401665bb12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TXT_FOLDER = \"/veld/input/\"\n",
    "OUTPUT_JSON_FOLDER = \"/veld/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877a6f7-9c25-44a9-9679-6711067a3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TEST_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587e65-e4a9-496d-971b-26f8c9ad52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conllu_data(conllu_file_path, pos_set_not_existing, feat_set_not_existing):\n",
    "    \"\"\"main function for conllu processing, takes repo_id and stem and returns conllu statistics\"\"\"\n",
    "    \n",
    "    # returned dictionary, defined here for communication of its structure\n",
    "    # data points from univesal dependencies are defined here: https://universaldependencies.org/format.html\n",
    "    result_dict = {\n",
    "        \"count_token\": 0,\n",
    "        \"count_lemma_total\": 0,\n",
    "        \"count_lemma_normalized_by_token\": 0,\n",
    "        # part of speech, as defined here: https://universaldependencies.org/u/pos/index.html\n",
    "        \"count_pos\": {\n",
    "            \"ADJ\": 0, \n",
    "            \"ADP\": 0, \n",
    "            \"ADV\": 0, \n",
    "            \"AUX\": 0, \n",
    "            \"CCONJ\": 0, \n",
    "            \"DET\": 0, \n",
    "            \"INTJ\": 0, \n",
    "            \"NOUN\": 0, \n",
    "            \"NUM\": 0, \n",
    "            \"PART\": 0, \n",
    "            \"PRON\": 0, \n",
    "            \"PROPN\": 0, \n",
    "            \"PUNCT\": 0,\n",
    "            \"SCONJ\": 0, \n",
    "            \"SYM\": 0, \n",
    "            \"VERB\": 0, \n",
    "            \"X\": 0,\n",
    "        },\n",
    "        # universal features, as defined here: https://universaldependencies.org/u/feat/index.html\n",
    "        \"count_feat\": {\n",
    "            # Lexical features\n",
    "            \"PronType\": 0,\n",
    "            \"NumType\": 0,\n",
    "            \"Poss\": 0,\n",
    "            \"Reflex\": 0,\n",
    "            \"Foreign\": 0,\n",
    "            \"Abbr\": 0,\n",
    "            \"Typo\": 0,\n",
    "            # Inflectional features, Nominal\n",
    "            \"Gender\": 0,\n",
    "            \"Animacy\": 0,\n",
    "            \"NounClass\": 0,\n",
    "            \"Number\": 0,\n",
    "            \"Case\": 0,\n",
    "            \"Definite\": 0,\n",
    "            \"Deixis\": 0,\n",
    "            \"DeixisRef\": 0,\n",
    "            \"Degree\": 0,\n",
    "            # Inflectional features, Verbal\n",
    "            \"VerbForm\": 0,\n",
    "            \"Mood\": 0,\n",
    "            \"Tense\": 0,\n",
    "            \"Aspect\": 0,\n",
    "            \"Voice\": 0,\n",
    "            \"Evident\": 0,\n",
    "            \"Polarity\": 0,\n",
    "            \"Person\": 0,\n",
    "            \"Polite\": 0,\n",
    "            \"Clusivity\": 0,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    lemma_set = set()\n",
    "    \n",
    "    # main loop over rows, create statistics\n",
    "    with open(conllu_file_path, \"r\") as f:\n",
    "        for row in csv.reader(f, delimiter=\"\\t\"):\n",
    "            if len(row) == 10 and not row[0].startswith(\"#\"):\n",
    "\n",
    "                # token\n",
    "                result_dict[\"count_token\"] = result_dict[\"count_token\"] + 1\n",
    "\n",
    "                # lemma\n",
    "                lemma_set.add(row[2])\n",
    "\n",
    "                # part of speech\n",
    "                pos = row[3]\n",
    "                if pos != \"_\":\n",
    "                    count_pos = result_dict[\"count_pos\"].get(pos)\n",
    "                    if count_pos is not None:\n",
    "                        result_dict[\"count_pos\"][pos] = count_pos + 1\n",
    "                    else:\n",
    "                        pos_set_not_existing.add(pos)\n",
    "\n",
    "                # universal features\n",
    "                feat_all = row[5]\n",
    "                if feat_all != \"_\":\n",
    "                    for feat_pair in feat_all.split(\"|\"):\n",
    "                        feat = feat_pair.split(\"=\")[0]\n",
    "                        count_feat = result_dict[\"count_feat\"].get(feat)\n",
    "                        if count_feat is not None:\n",
    "                            result_dict[\"count_feat\"][feat] = count_feat + 1\n",
    "                        else:\n",
    "                            feat_set_not_existing.add(feat)\n",
    "\n",
    "        # count lemmas from set and normalize\n",
    "        result_dict[\"count_lemma_total\"] = len(lemma_set)\n",
    "        if result_dict[\"count_token\"] != 0:\n",
    "            result_dict[\"count_lemma_normalized_by_token\"] = round(result_dict[\"count_lemma_total\"] / result_dict[\"count_token\"], 4)\n",
    "        else:\n",
    "            result_dict[\"count_lemma_normalized_by_token\"] = result_dict[\"count_lemma_total\"]\n",
    "    \n",
    "    return result_dict, pos_set_not_existing, feat_set_not_existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843b71c-38e6-49e4-8f49-5bea72d0334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TEST_RUN:\n",
    "    i_limit = 2\n",
    "    j_limit = 3\n",
    "else:\n",
    "    i_limit = None\n",
    "    j_limit = None\n",
    "\n",
    "# main loop over all eltec corpora\n",
    "for i, sub_folder in enumerate(os.listdir(INPUT_TXT_FOLDER)):\n",
    "\n",
    "    if i == i_limit:\n",
    "        break\n",
    "\n",
    "    if sub_folder == \".gitkeep\":\n",
    "        continue\n",
    "\n",
    "    # create data on corpus\n",
    "    eltec_corpus_id = sub_folder\n",
    "    input_conllu_sub_folder = INPUT_TXT_FOLDER + sub_folder + \"/level1\"\n",
    "    print(f\"processing eltec folder: {input_conllu_sub_folder}\")\n",
    "    output_conllu_json = []\n",
    "    pos_set_not_existing = set()\n",
    "    feat_set_not_existing = set()\n",
    "\n",
    "    # loop over files of corpus\n",
    "    for j, sub_file in enumerate(sorted(os.listdir(input_conllu_sub_folder))):\n",
    "\n",
    "        if j == j_limit:\n",
    "            break\n",
    "\n",
    "        # create data on file\n",
    "        eltec_resource_id = sub_file.replace(\".conllu\", \"\")\n",
    "        input_conllu_file = input_conllu_sub_folder + \"/\" + sub_file\n",
    "\n",
    "        # create stats\n",
    "        conllu_stats, pos_set_not_existing, feat_set_not_existing = process_conllu_data(\n",
    "            input_conllu_file, pos_set_not_existing, feat_set_not_existing)\n",
    "        if IS_TEST_RUN:\n",
    "            print(f\"processing eltec conllu file : {input_conllu_file}\")\n",
    "            print(conllu_stats)\n",
    "\n",
    "        # append results to json data carrier\n",
    "        output_conllu_json.append({\n",
    "            \"resource_uri\": f\"https://raw.githubusercontent.com/COST-ELTeC/{eltec_corpus_id}/master/level1/{eltec_resource_id}.xml\",\n",
    "            \"conllu_stats\": conllu_stats,\n",
    "        })\n",
    "\n",
    "    \n",
    "    # show missing / wrong data handling\n",
    "    print(f\"non-handled part-of-speech tags: {pos_set_not_existing}\")\n",
    "    print(f\"non-handled features: {feat_set_not_existing}\")\n",
    "\n",
    "    if not IS_TEST_RUN:\n",
    "        # write data of corpus into json file\n",
    "        output_json_file = OUTPUT_JSON_FOLDER + eltec_corpus_id + \".json\"\n",
    "        print(f\"writing results to {output_json_file}\")\n",
    "        with open(output_json_file, \"w\") as f:\n",
    "            json.dump(output_conllu_json, f, indent=4)\n",
    "\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
